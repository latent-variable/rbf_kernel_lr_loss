{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "## Due Sunday May 26, 2019 @ 11:59pm\n",
    "\n",
    "In this problem set, you will use *kernel* logistic regression and cross-validation for classifying spam (much as in the previous problem set, but now with kernels).\n",
    "You may *not* use scikit-learn or similar libraries, but only the linear algebra libraries of numpy.\n",
    "To that end, the code block below is the only \"imports\" you may have.  This block also loads the data.\n",
    "\n",
    "One change is that this code also \"z-normalizes\" the data (converts each feature into its z-score).  The same transformation is applied to the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "# plt.rcParams[\"figure.figsize\"] = (16, 9) # (w, h)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    trainX = np.loadtxt('spamtrainX.data')\n",
    "    trainY = np.loadtxt('spamtrainY.data')[:,np.newaxis]\n",
    "    mux = np.mean(trainX,axis=0)\n",
    "    trainX -= mux\n",
    "    stdx = np.std(trainX,axis=0)\n",
    "    trainX /= stdx\n",
    "\n",
    "    testX = np.loadtxt('spamtestX.data')\n",
    "    testX = (testX-mux)/stdx\n",
    "    testY = np.loadtxt('spamtestY.data')[:,np.newaxis]\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part a (4 points)\n",
    "\n",
    "First, we need to derive the kernel version of logistic regression.\n",
    "\n",
    "We will be optimizing\n",
    "\\begin{align*}\n",
    "L(f) &= \\sum_{i=1}^m l_{\\text{lr}}(y_i,f(x_i)) + \\frac{\\lambda}{2}\\|f\\|_{{\\cal H}_K}^2\n",
    "\\end{align*}\n",
    "where $l_{\\text{lr}}$ is the logistic regression loss from class and the last term is the norm of the function in the reproducing kernel Hilbert space with kernel $K$.\n",
    "\n",
    "From the Representer Theorem, we know that $f$ must have the following form\n",
    "\\begin{align*}\n",
    "f(x) &= \\sum_{j=1}^m \\alpha_j K(x_j,x)\n",
    "\\end{align*}\n",
    "\n",
    "Thus, our parameters are the $\\alpha$s, one for each training point.  Learning is the process of picking the parameters to minimize the total loss ($L$).  To this end, write down the loss function in terms of $\\alpha$ (the vector of all of the $\\alpha_j$ values).\n",
    "\n",
    "Note, $\\|f\\|_{{\\cal H}_K}^2 = \\alpha^\\top K\\alpha$. \n",
    "\n",
    "Let $K$ be a matrix for which $K_{ij} = K(x_i,x_j)$.  You may also wish to let $K_i$ be the $i$th column of the matrix $K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>__To Do: Place your derivation of $L$ in terms of $\\alpha$ below__</font>\n",
    "\\begin{align*}\n",
    "f(x_i) &= \\alpha_i K(x_i,x)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "l_{lr i} &=\\ln(1+e^{-y_if(x_i)})\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "L(f) &= \\sum_{i=1}^m -ln(1+e^{-y_i\\alpha_i K(x_i,x)} ) + \\frac{\\lambda}{2} \\alpha^\\top K\\alpha\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part b (4 points)\n",
    "\n",
    "To optimize this loss, we will just use gradient descent. (We could use Newton-Raphson, just like \"normal\" logistic regression, but for this assignment, we will keep it simpler by not needing the Hessian.)  To that end, derive the gradient of $L$ with respect to $\\alpha$ below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>__To Do: Place your derivation of $\\nabla_\\alpha L$ below__</font>\n",
    "\n",
    "\n",
    "$$ p_i = \\sigma (y_if(x_i))$$\n",
    "\n",
    "$$ p_i ' = p_i(1 - p_i)$$\n",
    "\n",
    "$$Gradient$$\n",
    "\n",
    "$$ \\nabla_\\alpha L(f) = \\sum_{i=1}^m \\nabla_\\alpha \\left( -ln(pi)  \\right)+ \\nabla_\\alpha \\left(\\frac{\\lambda}{2} \\alpha^\\top K\\alpha \\right) $$\n",
    "\n",
    "$$ \\nabla_\\alpha L = - \\left[\\sum_{i=1}^m (1-p_i)y_i K(x_i,x)\\right] + \\lambda K \\alpha$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part c (2 points)\n",
    "\n",
    "Compare this update rule on $\\alpha$ to the \"normal\" logistic regression update rule for $w$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>__To Do: place your comparison below__</font>\n",
    "\n",
    "$$\\nabla L = - \\left[\\sum_{i=1}^m (1-p_i)y_i x_i^\\top\\right] + \\lambda w $$\n",
    "\n",
    "$$ \\nabla_\\alpha L = - \\left[\\sum_{i=1}^m (1-p_i)y_i K(x_i,x)\\right] + \\lambda  K \\alpha$$\n",
    "\n",
    "$$\\alpha \\leftarrow \\alpha -  \\eta \\nabla_\\alpha L$$\n",
    "$$w \\leftarrow w - \\eta \\nabla_ w L$$\n",
    "\n",
    "The main difference is that $\\alpha$ is a vector with $m$ number of elements, the same number of elements as the training data. However, $w$ is a vector with $(n+1)$ number of elements, matching the corresponding features plus a bias term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part d (4 points)\n",
    "\n",
    "Below, write functions to calculate the loss function and its gradient (as you derived above).  I would recommend writing them in terms of the matrix $K$ (as opposed to the kernel function $K(\\cdot,\\cdot)$), because you don't want to reevaluate the kernel function on all of the training points each time you evaluate the loss function or its gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!! Your code here !!!!   \n",
    "def fn(a,K,Y,lmbda): \n",
    "    return (np.sum(np.log(1+np.exp(-Y*(K@a))),0) + (0.5*lmbda*np.sum(a.T@K@a)).T)[0]\n",
    "    \n",
    "def gradfn(a,K,Y,lmbda):\n",
    "    return -np.sum((1-1/(1+np.exp(-Y*(K@a))))*Y*K,0)[:,np.newaxis] + lmbda*K@a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part e (2 points)\n",
    "\n",
    "Below is an implementation of gradient descent, similar to that in PS 2, but with an adaptive step-size rule.\n",
    "\n",
    "Use this (and the functions you wrote above) to implement a function to learn the alpha weights, given a training X, a training Y, and a kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!! Your code here !!!\n",
    "#adaptive step-size\n",
    "#exp decay\n",
    "def step_size(eta_,k,t):\n",
    "    return eta_ *np.exp(-k*t)\n",
    "    \n",
    "\n",
    "# provided code for grad desc. eta is starting value. ittfn is optional function to call on each iteration\n",
    "def graddesc(w,fn,gradfn,eta = 0.1, ittfn=None):\n",
    "    eta_ = eta\n",
    "    oldf = fn(w)\n",
    "    df = 1\n",
    "    t = 0\n",
    "    k = .9\n",
    "    \n",
    "    while(df>1e-6):\n",
    "        g = gradfn(w)\n",
    "        t = 0\n",
    "        while eta>1e-10:\n",
    "            neww = w - eta*g\n",
    "            newf = fn(neww)\n",
    "            if oldf>newf*1.001:\n",
    "                break\n",
    "            eta = step_size(eta_,k,t)\n",
    "            t += 1\n",
    "#             print(eta)\n",
    "        if ittfn is not None:\n",
    "            ittfn(w,eta,newf)\n",
    "        if eta<=1e-10:\n",
    "            break\n",
    "        oldf = newf\n",
    "#         print(newf)\n",
    "        w = neww\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part f (4 points)\n",
    "\n",
    "Below is the \"squared exponential\" kernel, or the \"radial basis function\" kernel.  It takes a single parameter, $\\sigma$ which is the \"width\" of the kernel.\n",
    "\n",
    "Fix $\\lambda$ at $1$ (changing $\\lambda$ does not have a great effect in this case).  Use 3-fold cross-validation to find a good value of $\\sigma$, using the training set.  This is the same as in PS 2 where you were finding the best value of $\\lambda$ using cross-validation.  You may use the cross-validation code from your previous assignment or the solutions.\n",
    "\n",
    "Plot the cross-validation error as a function of $\\sigma$ for a range of sigmas from $0.1$ to $10$ on a semi-log plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up(x):\n",
    "    if len(x.shape)==1:\n",
    "        return x[np.newaxis,:]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def rbfkernel(x1,x2,sigma):\n",
    "    x1 = up(x1)\n",
    "    x2 = up(x2)\n",
    "    d = x1[:,np.newaxis,:] - x2[np.newaxis,:,:]\n",
    "    return np.exp(-np.sum(d*d,2)/(2*sigma*sigma))\n",
    "\n",
    "def trainlr(X,Y,sigma):\n",
    "    K = rbfkernel(X,X,sigma)\n",
    "    a = np.zeros((X.shape[0],1)) # starting w at zero works well for LR\n",
    "    return graddesc(a,lambda w : fn(w,K,Y,lmbda=1),\n",
    "                  lambda w : gradfn(w,K,Y,lmbda=1))\n",
    "\n",
    "def lrerrorrate(X,Y,a,sigma,trainX):\n",
    "    K = rbfkernel(X,trainX,sigma)\n",
    "    return np.sum(Y*np.sign(K@a)<0)/Y.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross validation error rate')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## !!! Your code for cross-validation and plotting here !!!\n",
    "def xvalideval(X,Y,sigma):\n",
    "    #print('sigma: ' +str(sigma))\n",
    "    nfold = 3\n",
    "    def evalfn(X,Y,eX,eY,sigma):\n",
    "        return lrerrorrate(eX,eY,trainlr(X,Y,sigma),sigma,X)     \n",
    "    m = X.shape[0]\n",
    "    splits = np.linspace(0,m,nfold+1).astype(int)\n",
    "    v = 0\n",
    "    i = 1\n",
    "    for low,high in zip(splits[0:-1],splits[1:]):\n",
    "        #print('fold: %d' %i)\n",
    "        i+=1\n",
    "        trainX = np.vstack((X[:low,:],X[high:,:]))\n",
    "        trainY = np.vstack((Y[:low,:],Y[high:,:]))\n",
    "        validX = X[low:high,:]\n",
    "        validY = Y[low:high,:]\n",
    "        v += evalfn(trainX,trainY,validX,validY,sigma)\n",
    "    return v /nfold\n",
    "\n",
    "\n",
    "trainX, trainY, testX, testY = load_data()\n",
    "# print(rbfkernel(trainX[1],trainX[0],sigma=1))\n",
    "ls = np.logspace(-1,1,10)\n",
    "xverr = ls.copy()\n",
    "bstv = None\n",
    "bsSig = None\n",
    "for i,l in enumerate(ls):\n",
    "    xverr[i] = xvalideval(trainX,trainY,l)\n",
    "    if bstv is None or bstv>xverr[i]:\n",
    "        bstv = xverr[i]\n",
    "        bsSig = l\n",
    "plt.semilogx(ls,xverr,'g-')\n",
    "plt.xlabel(\"sigma\")\n",
    "plt.ylabel(\"Cross validation error rate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part g (4 points)\n",
    "\n",
    "Select the value of $\\sigma$ that has the best cross-validation error.  Report the testing error, when using this value of $\\sigma$ to train on the entire training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 2.15443, test error = 0.0855715\n"
     ]
    }
   ],
   "source": [
    "### !!! Your code here to train and report error using best value of sigma !!!\n",
    "testerr = lrerrorrate(testX,testY,trainlr(trainX,trainY,bsSig),bsSig,trainX)\n",
    "print (\"sigma = %lg, test error = %lg\" % (bsSig,testerr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS 229 Python",
   "language": "python",
   "name": "cs229-cpu-limitation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
